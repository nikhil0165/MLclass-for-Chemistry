{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "good step\n",
      "{'x': array([0.15, 0.9 ]), 'evaluation': -0.9419937500000004, 'path': array([[1.5 , 1.5 ],\n",
      "       [0.15, 0.9 ]]), 'steps': 1}\n",
      "Time:  0.002016732993070036\n"
     ]
    }
   ],
   "source": [
    "#Q1 Local optimization using 1st and 2 order methods\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "\n",
    "\n",
    "def gradient(point):\n",
    "    x = point[0]\n",
    "    y= point[1]\n",
    "    ans = np.zeros(2)\n",
    "    ans[0] = 4*pow(x,3) - 2*x + 2*y\n",
    "    ans[1] = 2*x + 2*y\n",
    "    return ans\n",
    "\n",
    "def function(point):\n",
    "    x = point[0]\n",
    "    y= point[1]\n",
    "    return pow(x,4) - pow(x,2) + pow(y,2) + 2*x*y - 2\n",
    "\n",
    "def steepest_descent1a(func,first_derivative, starting_point, stepsize,tol = 1e-5):\n",
    "    direction = -first_derivative(starting_point)\n",
    "    count = 0\n",
    "    visited = starting_point\n",
    "    ld = stepsize\n",
    "    print(ld)\n",
    "    old_point = starting_point\n",
    "    while np.linalg.norm(first_derivative(old_point)) > tol and count<1:\n",
    "        new_point = old_point + np.multiply(ld,direction)\n",
    "        visited = np.vstack((visited,new_point))\n",
    "        if func(new_point) < func(old_point):\n",
    "            ld = 1.2*ld\n",
    "            old_point = new_point\n",
    "            direction = -first_derivative(old_point)\n",
    "            print(\"good step\")\n",
    "        else:\n",
    "            ld = 0.5*ld\n",
    "            print(\"bad step\")\n",
    "        count += 1\n",
    "    return {\"x\": old_point, \"evaluation\": func(old_point), \"path\": visited, \"steps\": count}\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "ans = steepest_descent1a(function,gradient,np.array([1.5,1.5]),0.1,1)\n",
    "print(ans)\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1a As it turns out this is a good optimization step, the function value at new point is lesser. Since we are going in the minima we should increase the step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "{'x': array([-0.99999852,  0.99999607]), 'evaluation': -2.999999999985186, 'path': array([[ 1.5       ,  1.5       ],\n",
      "       [ 0.15      ,  0.9       ],\n",
      "       [-0.03162   ,  0.648     ],\n",
      "       [-0.22733235,  0.47048256],\n",
      "       [-0.4603766 ,  0.38644985],\n",
      "       [-0.73063964,  0.41710875],\n",
      "       [-0.91361447,  0.57314178],\n",
      "       [-0.89067253,  0.77647099],\n",
      "       [-1.072703  ,  0.85831194],\n",
      "       [-0.98168777,  0.81739147],\n",
      "       [-0.94167921,  0.88803587],\n",
      "       [-1.0240441 ,  0.91571465],\n",
      "       [-0.89525453,  0.98278939],\n",
      "       [-0.95964931,  0.94925202],\n",
      "       [-1.01216804,  0.95311466],\n",
      "       [-0.96374581,  0.97944095],\n",
      "       [-0.98795692,  0.96627781],\n",
      "       [-0.99481157,  0.9720766 ],\n",
      "       [-0.99412388,  0.97937406],\n",
      "       [-0.99741632,  0.98505533],\n",
      "       [-0.99646125,  0.99076871],\n",
      "       [-1.00111334,  0.9939261 ],\n",
      "       [-0.99336059,  0.99870981],\n",
      "       [-0.99723697,  0.99631795],\n",
      "       [-1.00126535,  0.99668496],\n",
      "       [-0.9966402 ,  0.99887998],\n",
      "       [-0.99895278,  0.99778247],\n",
      "       [-0.99981882,  0.99811897],\n",
      "       [-0.99948229,  0.99870549],\n",
      "       [-1.00001741,  0.99902712],\n",
      "       [-0.99949077,  0.99951915],\n",
      "       [-0.99975409,  0.99927314],\n",
      "       [-0.99990384,  0.99941651],\n",
      "       [-0.99986709,  0.99959085],\n",
      "       [-0.99997668,  0.99970943],\n",
      "       [-0.99988705,  0.9998471 ],\n",
      "       [-1.00014159,  0.9998718 ],\n",
      "       [-1.00001432,  0.99985945],\n",
      "       [-0.99993563,  0.99991689],\n",
      "       [-1.00004188,  0.99992523],\n",
      "       [-0.99998875,  0.99992106],\n",
      "       [-0.99998269,  0.99993914],\n",
      "       [-0.99999092,  0.9999531 ],\n",
      "       [-0.99999034,  0.99996764],\n",
      "       [-0.9999977 ,  0.99997812],\n",
      "       [-0.99999196,  0.99998896],\n",
      "       [-1.00001135,  0.99999095],\n",
      "       [-1.00000165,  0.99998995],\n",
      "       [-0.99999435,  0.99999462],\n",
      "       [-1.00000528,  0.99999449],\n",
      "       [-0.99999982,  0.99999455],\n",
      "       [-0.99999852,  0.99999607]]), 'steps': 51}\n",
      "Time:  0.003131937002763152\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def steepest_descent(func,first_derivative, starting_point, stepsize,tol = 1e-5):\n",
    "    direction = -first_derivative(starting_point)\n",
    "    count = 0\n",
    "    visited = starting_point\n",
    "    ld = stepsize\n",
    "    print(ld)\n",
    "    old_point = starting_point\n",
    "    while np.linalg.norm(first_derivative(old_point)) > tol and count<1e6:\n",
    "        new_point = old_point + np.multiply(ld,direction)\n",
    "        visited = np.vstack((visited,new_point))\n",
    "        if func(new_point) < func(old_point):\n",
    "            ld = 1.2*ld\n",
    "            old_point = new_point\n",
    "            direction = -first_derivative(old_point)\n",
    "        else:\n",
    "            ld = 0.5*ld\n",
    "        count += 1\n",
    "    return {\"x\": old_point, \"evaluation\": func(old_point), \"path\": visited, \"steps\": count}\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "ans = steepest_descent(function,gradient,np.array([1.5,1.5]),0.1)\n",
    "print(ans)\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1b It takes 51 steps and 0.0024 secs for my code to converge. Superficially my answer might look different from yours but if you look at the array of visited points, my code has traversed your minima and has gone to even a lower point. My guess is that since we are dealing with very small numbers small truncation errors corresponding to different machines can lead to slightly different answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "{'x': array([-0.99999852,  0.99999607]), 'evaluation': -2.999999999985186, 'path': array([[ 1.5       ,  1.5       ],\n",
      "       [ 0.15      ,  0.9       ],\n",
      "       [-0.03162   ,  0.648     ],\n",
      "       [-0.22733235,  0.47048256],\n",
      "       [-0.4603766 ,  0.38644985],\n",
      "       [-0.73063964,  0.41710875],\n",
      "       [-0.91361447,  0.57314178],\n",
      "       [-0.89067253,  0.77647099],\n",
      "       [-1.072703  ,  0.85831194],\n",
      "       [-0.98168777,  0.81739147],\n",
      "       [-0.94167921,  0.88803587],\n",
      "       [-1.0240441 ,  0.91571465],\n",
      "       [-0.89525453,  0.98278939],\n",
      "       [-0.95964931,  0.94925202],\n",
      "       [-1.01216804,  0.95311466],\n",
      "       [-0.96374581,  0.97944095],\n",
      "       [-0.98795692,  0.96627781],\n",
      "       [-0.99481157,  0.9720766 ],\n",
      "       [-0.99412388,  0.97937406],\n",
      "       [-0.99741632,  0.98505533],\n",
      "       [-0.99646125,  0.99076871],\n",
      "       [-1.00111334,  0.9939261 ],\n",
      "       [-0.99336059,  0.99870981],\n",
      "       [-0.99723697,  0.99631795],\n",
      "       [-1.00126535,  0.99668496],\n",
      "       [-0.9966402 ,  0.99887998],\n",
      "       [-0.99895278,  0.99778247],\n",
      "       [-0.99981882,  0.99811897],\n",
      "       [-0.99948229,  0.99870549],\n",
      "       [-1.00001741,  0.99902712],\n",
      "       [-0.99949077,  0.99951915],\n",
      "       [-0.99975409,  0.99927314],\n",
      "       [-0.99990384,  0.99941651],\n",
      "       [-0.99986709,  0.99959085],\n",
      "       [-0.99997668,  0.99970943],\n",
      "       [-0.99988705,  0.9998471 ],\n",
      "       [-1.00014159,  0.9998718 ],\n",
      "       [-1.00001432,  0.99985945],\n",
      "       [-0.99993563,  0.99991689],\n",
      "       [-1.00004188,  0.99992523],\n",
      "       [-0.99998875,  0.99992106],\n",
      "       [-0.99998269,  0.99993914],\n",
      "       [-0.99999092,  0.9999531 ],\n",
      "       [-0.99999034,  0.99996764],\n",
      "       [-0.9999977 ,  0.99997812],\n",
      "       [-0.99999196,  0.99998896],\n",
      "       [-1.00001135,  0.99999095],\n",
      "       [-1.00000165,  0.99998995],\n",
      "       [-0.99999435,  0.99999462],\n",
      "       [-1.00000528,  0.99999449],\n",
      "       [-0.99999982,  0.99999455],\n",
      "       [-0.99999852,  0.99999607]]), 'steps': 51}\n",
      "Time_SD:  0.00182876898907125\n",
      "     fun: -2.9999999999995843\n",
      "     jac: array([ 2.08616257e-07, -1.10268593e-06])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 78\n",
      "     nit: 9\n",
      "    njev: 26\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-0.99999984,  0.99999928])\n",
      "Time_CG:  0.0037078040186315775\n",
      "      fun: -2.9999999999998184\n",
      " hess_inv: array([[ 0.12457747, -0.12457762],\n",
      "       [-0.12457762,  0.62569996]])\n",
      "      jac: array([-1.63912773e-06, -2.98023224e-08])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 24\n",
      "      nit: 7\n",
      "     njev: 8\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 0.99999979, -0.99999979])\n",
      "Time_BFGS:  0.027161831967532635\n"
     ]
    }
   ],
   "source": [
    "import scipy.optimize as optm\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "ans = steepest_descent(function,gradient,np.array([1.5,1.5]),0.1)\n",
    "print(ans)\n",
    "stop = timeit.default_timer()\n",
    "print('Time_SD: ', stop - start)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "ans_CG = optm.minimize(function,np.array([1.5,1.5]),method = 'CG')\n",
    "print(ans_CG)\n",
    "stop = timeit.default_timer()\n",
    "print('Time_CG: ', stop - start)\n",
    "\n",
    "\n",
    "start = timeit.default_timer()\n",
    "ans_BFGS  = optm.minimize(function,np.array([1.5,1.5]),method = 'BFGS')\n",
    "print(ans_BFGS)\n",
    "stop = timeit.default_timer()\n",
    "print('Time_BFGS: ', stop - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1c From the above calculation it seems like BFGS is the fastest method with only 7 steps, CG takes 9 steps whereas SD takes significantly larger 51 steps. So yes BFGS and CG are significantly faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Local Optimization using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "{'x': array([0.99999089, 0.99998153]), 'evaluation': 8.361266796946337e-11, 'path': array([[-0.5       ,  1.5       ],\n",
      "       [-2.7       , -1.        ],\n",
      "       [-1.6       ,  0.25      ],\n",
      "       ...,\n",
      "       [ 0.99999119,  0.99998136],\n",
      "       [ 0.99999093,  0.99998135],\n",
      "       [ 0.99999089,  0.99998153]]), 'steps': 1523}\n",
      "Time_SD:  0.07237392803654075\n"
     ]
    }
   ],
   "source": [
    "# First the definitions of Rosenbrock functions\n",
    "\n",
    "def rb_gradient(point):\n",
    "    x = point[0]\n",
    "    y = point[1]\n",
    "    ans =np.zeros(2)\n",
    "    ans[0] = 2*(x-1) + 40*x*(x**2 -y)\n",
    "    ans[1] = 20*(y-x**2)\n",
    "    return ans\n",
    "\n",
    "def rb_function(point):\n",
    "    x = point[0]\n",
    "    y = point[1]\n",
    "    return (1-x)**2 + 10*((y-x**2)**2)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "ans = steepest_descent(rb_function,rb_gradient,np.array([-0.5,1.5]),0.1)\n",
    "print(ans)\n",
    "stop = timeit.default_timer()\n",
    "print('Time_SD: ', stop - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2a My SD code took ~1500 steps to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': array([0.999989  , 0.99997757]), 'evaluation': 1.2283883379385878e-10, 'steps': 2168}\n",
      "Time:  0.0752481420058757\n"
     ]
    }
   ],
   "source": [
    "def stochastic_gradient_descent(func,first_derivative,starting_point,stepsize,tol=1e-5,stochastic_injection=1):\n",
    "    count = 0\n",
    "    visited = starting_point\n",
    "    slope = first_derivative(starting_point)\n",
    "    ld = stepsize\n",
    "    old_point = starting_point\n",
    "    while np.linalg.norm(slope) > tol and count<1e6:\n",
    "        if stochastic_injection > 0:\n",
    "             vec = np.random.random(len(starting_point)) - 0.5\n",
    "             vec = np.true_divide(vec,np.linalg.norm(vec))\n",
    "             stochastic_deriv = np.multiply(vec, np.linalg.norm(slope))\n",
    "        else:\n",
    "             stochastic_deriv = np.zeros(len(starting_point))\n",
    "        direction = slope + stochastic_injection*stochastic_deriv\n",
    "        new_point = old_point - np.multiply(ld,direction)\n",
    "        visited = np.vstack((visited,new_point))\n",
    "        if func(new_point) < func(old_point):\n",
    "            ld = 1.2 * ld\n",
    "            old_point = new_point\n",
    "            slope = first_derivative(old_point)\n",
    "        else:\n",
    "            ld = 0.5 * ld\n",
    "        count += 1\n",
    "    return {\"x\": old_point, \"evaluation\": func(old_point), \"steps\": count}\n",
    "\n",
    "\n",
    "start = timeit.default_timer()\n",
    "ans2b = stochastic_gradient_descent(rb_function,rb_gradient,np.array([-0.5,1.5]),0.1)\n",
    "print(ans2b)\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2b My code took around ~2200 steps to converge, somewhat larger than SD. I guess we see more steps in SGD because of the random perturbation we add to the gradient. This perturbations are useful when we want to optimize a different function using Rosenbrock as a learning system. But if we want to optimize Rosenbrock only it is better to use exact gradients, that is steepest descent method.\n",
    "\n",
    "Also for visual clarity I have stopped printing VISITED array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': array([0.99998986, 0.99997928]), 'evaluation': 1.0475113535428185e-10, 'steps': 2394}\n",
      "Time_SGD:  0.15187846397748217\n",
      "     fun: 2.0711814827200667e-13\n",
      "     jac: array([ 4.94555024e-08, -2.45172016e-08])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 132\n",
      "     nit: 20\n",
      "    njev: 44\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.99999955, 0.99999908])\n",
      "Time_CG:  0.006265747011639178\n",
      "      fun: 1.6856836004019217e-13\n",
      " hess_inv: array([[0.50988602, 1.01962714],\n",
      "       [1.01962714, 2.08896666]])\n",
      "      jac: array([ 1.15312325e-07, -1.29424893e-08])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 93\n",
      "      nit: 22\n",
      "     njev: 31\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([0.99999959, 0.99999917])\n",
      "Time_BFGS:  0.0051363150123506784\n"
     ]
    }
   ],
   "source": [
    "import scipy.optimize as optm\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "ans = stochastic_gradient_descent(rb_function,rb_gradient,np.array([-0.5,1.5]),0.1)\n",
    "print(ans)\n",
    "stop = timeit.default_timer()\n",
    "print('Time_SGD: ', stop - start)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "ans_CG = optm.minimize(rb_function,np.array([-0.5,1.5]),method = 'CG')\n",
    "print(ans_CG)\n",
    "stop = timeit.default_timer()\n",
    "print('Time_CG: ', stop - start)\n",
    "\n",
    "\n",
    "start = timeit.default_timer()\n",
    "ans_BFGS  = optm.minimize(rb_function,np.array([-0.5,1.5]),method = 'BFGS')\n",
    "print(ans_BFGS)\n",
    "stop = timeit.default_timer()\n",
    "print('Time_BFGS: ', stop - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2c BFGS takes 22 steps, CG 20 steps and SGD ~2200 steps. Here also CG and BFGS perform much better than SGD. CG being the best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2d In principle we cannot judge performance of a stochastic method by one run. However the difference between SGD and CG, BFGS is so huge that I am sure that even after statistical averaging CG and BFGS will perform much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2e The relative performance of non stochastic method should be better as we will be using the correct gradients to solve for minima. Adding a random vector is more likely to take us away from minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3 Stochastic Gradient method with momentum (SGDM)\n",
    "\n",
    "# first the definitions of three-hump-camel function\n",
    "\n",
    "def thc_gradient(point):\n",
    "    x = point[0]\n",
    "    y= point[1]\n",
    "    ans = np.zeros(2)\n",
    "    ans[0] = 4*x - 4.2*pow(x,3) + pow(x,5) +y\n",
    "    ans[1] = x + 2*y\n",
    "    return ans\n",
    "\n",
    "def thc_function(point):\n",
    "    x= point[0]\n",
    "    y= point[1]\n",
    "    return 2*pow(x,2) - 1.05*pow(x,4) + (1/6)*pow(x,6) + x*y + pow(y,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(func,first_derivative,starting_point,stepsize,tol=1e-5,stochastic_injection=1):\n",
    "    count = 0\n",
    "    visited = starting_point\n",
    "    slope = first_derivative(starting_point)\n",
    "    ld = stepsize\n",
    "    old_point = starting_point\n",
    "    func_old = func(starting_point)\n",
    "    func_new = func(starting_point)\n",
    "    while not (not (np.linalg.norm(slope) > tol) and not (abs(func_new - func_old) > tol and count < 1e6)):\n",
    "        if stochastic_injection > 0:\n",
    "             vec = np.random.random(len(starting_point)) - 0.5\n",
    "             vec = np.true_divide(vec,np.linalg.norm(vec))\n",
    "             stochastic_deriv = np.multiply(vec, np.linalg.norm(slope))\n",
    "        else:\n",
    "             stochastic_deriv = np.zeros(len(starting_point))\n",
    "        direction = slope + stochastic_injection*stochastic_deriv\n",
    "        new_point = old_point - np.multiply(ld,direction)\n",
    "        visited = np.vstack((visited,new_point))\n",
    "        if func(new_point) < func(old_point):\n",
    "            func_new = func(new_point)\n",
    "            func_old = func(old_point)\n",
    "            ld = 1.2 * ld\n",
    "            old_point = new_point\n",
    "            slope = first_derivative(old_point)\n",
    "        else:\n",
    "            ld = 0.5 * ld\n",
    "        count += 1\n",
    "    return {\"x\": old_point, \"evaluation\": func(old_point), \"steps\": count}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGDM(func,first_derivative,starting_point,stepsize,momentum=0.9,tol=1e-5,stochastic_injection =1):\n",
    "    count = 0\n",
    "    visited = starting_point\n",
    "    slope = first_derivative(starting_point)\n",
    "    direction = np.copy(slope)\n",
    "    ld = stepsize\n",
    "    old_point = starting_point\n",
    "    func_old = func(starting_point)\n",
    "    func_new = func(starting_point)\n",
    "    while not (not (np.linalg.norm(slope) > tol) and not (abs(func_new - func_old) > tol and count < 1e6)):\n",
    "        if stochastic_injection > 0:\n",
    "             vec = np.random.random(len(starting_point)) -0.5#\n",
    "             vec = np.true_divide(vec,np.linalg.norm(vec))\n",
    "             stochastic_deriv = np.multiply(vec, np.linalg.norm(slope))\n",
    "        else:\n",
    "             stochastic_deriv = np.zeros(len(starting_point))\n",
    "        direction = slope + stochastic_injection*stochastic_deriv + momentum*direction\n",
    "        new_point = old_point - np.multiply(ld,direction)\n",
    "        visited = np.vstack((visited,new_point))\n",
    "        if func(new_point) < func(old_point):\n",
    "            func_new = func(new_point)\n",
    "            func_old = func(old_point)\n",
    "            ld = 1.2 * ld\n",
    "            old_point = new_point\n",
    "            slope = first_derivative(old_point)\n",
    "        else:\n",
    "            ld = 0.5 * ld\n",
    "        count += 1\n",
    "    return {\"x\": old_point, \"evaluation\": func(old_point), \"steps\": count}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': array([1.19855024e-06, 1.95410252e-07]), 'evaluation': 3.1454395054518724e-12, 'steps': 44}\n",
      "Time_SGD:  0.0020014640176668763\n",
      "{'x': array([-1.74755269,  0.87378108]), 'evaluation': 0.2986384422599382, 'steps': 234}\n",
      "Time_SGDM:  0.013834834971930832\n",
      "     fun: 0.2986384422397135\n",
      "     jac: array([8.44895840e-06, 7.15255737e-07])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 63\n",
      "     nit: 7\n",
      "    njev: 21\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-1.74755166,  0.87377618])\n",
      "Time_CG:  0.003156670951284468\n",
      "      fun: 0.29863844223686\n",
      " hess_inv: array([[ 0.08568879, -0.04290027],\n",
      "       [-0.04290027,  0.51091277]])\n",
      "      jac: array([1.34110451e-07, 0.00000000e+00])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 30\n",
      "      nit: 8\n",
      "     njev: 10\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([-1.74755234,  0.87377616])\n",
      "Time_BFGS:  0.0021103890030644834\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "ans = SGD(thc_function,thc_gradient,np.array([-1.5,-1.5]),0.1)\n",
    "print(ans)\n",
    "stop = timeit.default_timer()\n",
    "print('Time_SGD: ', stop - start)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "ans = SGDM(thc_function,thc_gradient,np.array([-1.5,-1.5]),0.1)\n",
    "print(ans)\n",
    "stop = timeit.default_timer()\n",
    "print('Time_SGDM: ', stop - start)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "ans_CG= optm.minimize(thc_function,np.array([-1.5,-1.5]),method = 'CG')\n",
    "print(ans_CG)\n",
    "stop = timeit.default_timer()\n",
    "print('Time_CG: ', stop - start)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "ans_BFGS  = optm.minimize(thc_function,np.array([-1.5,-1.5]),method = 'BFGS')\n",
    "print(ans_BFGS)\n",
    "stop = timeit.default_timer()\n",
    "print('Time_BFGS: ', stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
